# -*- coding: utf-8 -*-
"""Welcome to Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import joblib

# Load and preprocess data
df = pd.read_csv('/content/coralreef_dataset.csv')
df = df.dropna(subset=['Percent_Bleaching'])

# Feature engineering
features = ['Cyclone_Frequency', 'Depth_m', 'ClimSST', 'Turbidity',
            'Temperature_Maximum', 'SSTA', 'TSA', 'Temperature_Mean']
target = 'Percent_Bleaching'

# Handle categorical variables
df = pd.get_dummies(df, columns=['Ocean_Name', 'Country_Name'])

# Split data
X = df[features]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save scaler
joblib.dump(scaler, 'scaler.pkl')

# XGBoost Model
xgb = XGBRegressor(objective='reg:squarederror')
xgb.fit(X_train_scaled, y_train)
joblib.dump(xgb, 'xgb_model.pkl')

# Random Forest Model
rf = RandomForestRegressor(n_estimators=100)
rf.fit(X_train_scaled, y_train)
joblib.dump(rf, 'rf_model.pkl')

# LSTM Model
X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

lstm_model = Sequential()
lstm_model.add(LSTM(64, activation='relu', input_shape=(X_train_lstm.shape[1], 1), return_sequences=True))
lstm_model.add(LSTM(32, activation='relu'))
lstm_model.add(Dense(1))
lstm_model.compile(optimizer='adam', loss='mse')

lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, validation_split=0.2)
lstm_model.save('lstm_model.h5')

# Evaluate models
def evaluate_model(model, X, y):
    preds = model.predict(X)
    # Convert predictions to numpy array and replace NaN with a finite number
    preds = np.nan_to_num(preds.astype(np.float64))
    return {
        'R2': r2_score(y, preds),
        'MAE': mean_absolute_error(y, preds)
    }

print("XGBoost Performance:", evaluate_model(xgb, X_test_scaled, y_test))
print("Random Forest Performance:", evaluate_model(rf, X_test_scaled, y_test))
print("LSTM Performance:", evaluate_model(lstm_model, X_test_lstm, y_test))